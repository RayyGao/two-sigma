{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_data=pd.read_json(\"../data/processed_train.json\")\n",
    "img=pd.read_csv(\"../data/image_stats-fixed.csv\",index_col=0)\n",
    "processed_data=processed_data.merge(img,how=\"left\",on=\"listing_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_data=processed_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_data=processed_data.sample(n=processed_data.shape[0]*7/10)\n",
    "test_data=processed_data.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################first try with all numeric features in datasets##########################################\n",
    "train=train_data.drop(['building_id','created','description','display_address','longitude','latitude','manager count','manager_id','listing_id','photos','street_address','features'],axis=1)\n",
    "test=test_data.drop(['building_id','created','description','display_address','manager count','longitude','latitude','manager_id','listing_id','photos','street_address','features'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#base model 1: multinomial logistic regression\n",
    "#base model 2: bagged decision trees\n",
    "#base model 3: Random Forest trees\n",
    "#base model 4: SVM\n",
    "#base model 5: bayes classifier\n",
    "#base model 6: Ada Boosting\n",
    "y_train=train.loc[:,'interest_level']\n",
    "x_train=train.drop('interest_level',axis=1)\n",
    "y_test=test.loc[:,'interest_level']\n",
    "x_test=test.drop('interest_level',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diction={'low':0,'medium':1,'high':2}\n",
    "y_train1=map(lambda x: diction[x],y_train)\n",
    "y_test1=map(lambda x: diction[x],y_test)\n",
    "y_train2=pd.Series(y_train1,index=y_train.index)\n",
    "y_test2=pd.Series(y_test1,index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr(x_train,y_train,x_test):\n",
    "\tlr=LogisticRegression(solver='lbfgs',multi_class='multinomial')\n",
    "\tlr.fit(x_train,y_train)\n",
    "\treturn lr.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr_low</th>\n",
       "      <th>lr_medium</th>\n",
       "      <th>lr_high</th>\n",
       "      <th>lr_low</th>\n",
       "      <th>lr_medium</th>\n",
       "      <th>lr_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.754752</td>\n",
       "      <td>0.199560</td>\n",
       "      <td>0.045688</td>\n",
       "      <td>0.754752</td>\n",
       "      <td>0.199560</td>\n",
       "      <td>0.045688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.767030</td>\n",
       "      <td>0.191980</td>\n",
       "      <td>0.040989</td>\n",
       "      <td>0.767030</td>\n",
       "      <td>0.191980</td>\n",
       "      <td>0.040989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.722733</td>\n",
       "      <td>0.209716</td>\n",
       "      <td>0.067551</td>\n",
       "      <td>0.722733</td>\n",
       "      <td>0.209716</td>\n",
       "      <td>0.067551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.606957</td>\n",
       "      <td>0.273783</td>\n",
       "      <td>0.119260</td>\n",
       "      <td>0.606957</td>\n",
       "      <td>0.273783</td>\n",
       "      <td>0.119260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.605158</td>\n",
       "      <td>0.274803</td>\n",
       "      <td>0.120040</td>\n",
       "      <td>0.605158</td>\n",
       "      <td>0.274803</td>\n",
       "      <td>0.120040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.568870</td>\n",
       "      <td>0.331278</td>\n",
       "      <td>0.099852</td>\n",
       "      <td>0.568870</td>\n",
       "      <td>0.331278</td>\n",
       "      <td>0.099852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.580670</td>\n",
       "      <td>0.287763</td>\n",
       "      <td>0.131567</td>\n",
       "      <td>0.580670</td>\n",
       "      <td>0.287763</td>\n",
       "      <td>0.131567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.763173</td>\n",
       "      <td>0.196542</td>\n",
       "      <td>0.040285</td>\n",
       "      <td>0.763173</td>\n",
       "      <td>0.196542</td>\n",
       "      <td>0.040285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.788089</td>\n",
       "      <td>0.175586</td>\n",
       "      <td>0.036325</td>\n",
       "      <td>0.788089</td>\n",
       "      <td>0.175586</td>\n",
       "      <td>0.036325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.706143</td>\n",
       "      <td>0.227630</td>\n",
       "      <td>0.066228</td>\n",
       "      <td>0.706143</td>\n",
       "      <td>0.227630</td>\n",
       "      <td>0.066228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.648618</td>\n",
       "      <td>0.239978</td>\n",
       "      <td>0.111404</td>\n",
       "      <td>0.648618</td>\n",
       "      <td>0.239978</td>\n",
       "      <td>0.111404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.705901</td>\n",
       "      <td>0.235113</td>\n",
       "      <td>0.058986</td>\n",
       "      <td>0.705901</td>\n",
       "      <td>0.235113</td>\n",
       "      <td>0.058986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.619465</td>\n",
       "      <td>0.266144</td>\n",
       "      <td>0.114391</td>\n",
       "      <td>0.619465</td>\n",
       "      <td>0.266144</td>\n",
       "      <td>0.114391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.440288</td>\n",
       "      <td>0.366893</td>\n",
       "      <td>0.192819</td>\n",
       "      <td>0.440288</td>\n",
       "      <td>0.366893</td>\n",
       "      <td>0.192819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.450812</td>\n",
       "      <td>0.333215</td>\n",
       "      <td>0.215973</td>\n",
       "      <td>0.450812</td>\n",
       "      <td>0.333215</td>\n",
       "      <td>0.215973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.589071</td>\n",
       "      <td>0.284220</td>\n",
       "      <td>0.126708</td>\n",
       "      <td>0.589071</td>\n",
       "      <td>0.284220</td>\n",
       "      <td>0.126708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.499611</td>\n",
       "      <td>0.354675</td>\n",
       "      <td>0.145714</td>\n",
       "      <td>0.499611</td>\n",
       "      <td>0.354675</td>\n",
       "      <td>0.145714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.480811</td>\n",
       "      <td>0.337268</td>\n",
       "      <td>0.181921</td>\n",
       "      <td>0.480811</td>\n",
       "      <td>0.337268</td>\n",
       "      <td>0.181921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.776923</td>\n",
       "      <td>0.194684</td>\n",
       "      <td>0.028393</td>\n",
       "      <td>0.776923</td>\n",
       "      <td>0.194684</td>\n",
       "      <td>0.028393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.762961</td>\n",
       "      <td>0.205669</td>\n",
       "      <td>0.031370</td>\n",
       "      <td>0.762961</td>\n",
       "      <td>0.205669</td>\n",
       "      <td>0.031370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.529278</td>\n",
       "      <td>0.331641</td>\n",
       "      <td>0.139082</td>\n",
       "      <td>0.529278</td>\n",
       "      <td>0.331641</td>\n",
       "      <td>0.139082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.627941</td>\n",
       "      <td>0.294975</td>\n",
       "      <td>0.077084</td>\n",
       "      <td>0.627941</td>\n",
       "      <td>0.294975</td>\n",
       "      <td>0.077084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.893475</td>\n",
       "      <td>0.100422</td>\n",
       "      <td>0.006104</td>\n",
       "      <td>0.893475</td>\n",
       "      <td>0.100422</td>\n",
       "      <td>0.006104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.640884</td>\n",
       "      <td>0.282798</td>\n",
       "      <td>0.076318</td>\n",
       "      <td>0.640884</td>\n",
       "      <td>0.282798</td>\n",
       "      <td>0.076318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.517706</td>\n",
       "      <td>0.324035</td>\n",
       "      <td>0.158258</td>\n",
       "      <td>0.517706</td>\n",
       "      <td>0.324035</td>\n",
       "      <td>0.158258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.695699</td>\n",
       "      <td>0.240099</td>\n",
       "      <td>0.064202</td>\n",
       "      <td>0.695699</td>\n",
       "      <td>0.240099</td>\n",
       "      <td>0.064202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.559610</td>\n",
       "      <td>0.326922</td>\n",
       "      <td>0.113468</td>\n",
       "      <td>0.559610</td>\n",
       "      <td>0.326922</td>\n",
       "      <td>0.113468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.629339</td>\n",
       "      <td>0.260121</td>\n",
       "      <td>0.110541</td>\n",
       "      <td>0.629339</td>\n",
       "      <td>0.260121</td>\n",
       "      <td>0.110541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.590608</td>\n",
       "      <td>0.300260</td>\n",
       "      <td>0.109133</td>\n",
       "      <td>0.590608</td>\n",
       "      <td>0.300260</td>\n",
       "      <td>0.109133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.503730</td>\n",
       "      <td>0.328242</td>\n",
       "      <td>0.168028</td>\n",
       "      <td>0.503730</td>\n",
       "      <td>0.328242</td>\n",
       "      <td>0.168028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49267</th>\n",
       "      <td>0.565790</td>\n",
       "      <td>0.303253</td>\n",
       "      <td>0.130957</td>\n",
       "      <td>0.565790</td>\n",
       "      <td>0.303253</td>\n",
       "      <td>0.130957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49269</th>\n",
       "      <td>0.594715</td>\n",
       "      <td>0.279239</td>\n",
       "      <td>0.126046</td>\n",
       "      <td>0.594715</td>\n",
       "      <td>0.279239</td>\n",
       "      <td>0.126046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49271</th>\n",
       "      <td>0.491012</td>\n",
       "      <td>0.336680</td>\n",
       "      <td>0.172308</td>\n",
       "      <td>0.491012</td>\n",
       "      <td>0.336680</td>\n",
       "      <td>0.172308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49276</th>\n",
       "      <td>0.627381</td>\n",
       "      <td>0.271441</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>0.627381</td>\n",
       "      <td>0.271441</td>\n",
       "      <td>0.101177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49277</th>\n",
       "      <td>0.839203</td>\n",
       "      <td>0.140196</td>\n",
       "      <td>0.020601</td>\n",
       "      <td>0.839203</td>\n",
       "      <td>0.140196</td>\n",
       "      <td>0.020601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49278</th>\n",
       "      <td>0.472583</td>\n",
       "      <td>0.361601</td>\n",
       "      <td>0.165816</td>\n",
       "      <td>0.472583</td>\n",
       "      <td>0.361601</td>\n",
       "      <td>0.165816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49279</th>\n",
       "      <td>0.514369</td>\n",
       "      <td>0.308372</td>\n",
       "      <td>0.177258</td>\n",
       "      <td>0.514369</td>\n",
       "      <td>0.308372</td>\n",
       "      <td>0.177258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49281</th>\n",
       "      <td>0.698104</td>\n",
       "      <td>0.228922</td>\n",
       "      <td>0.072974</td>\n",
       "      <td>0.698104</td>\n",
       "      <td>0.228922</td>\n",
       "      <td>0.072974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49284</th>\n",
       "      <td>0.686001</td>\n",
       "      <td>0.243596</td>\n",
       "      <td>0.070404</td>\n",
       "      <td>0.686001</td>\n",
       "      <td>0.243596</td>\n",
       "      <td>0.070404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49286</th>\n",
       "      <td>0.956840</td>\n",
       "      <td>0.042004</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.956840</td>\n",
       "      <td>0.042004</td>\n",
       "      <td>0.001156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49293</th>\n",
       "      <td>0.699132</td>\n",
       "      <td>0.235778</td>\n",
       "      <td>0.065090</td>\n",
       "      <td>0.699132</td>\n",
       "      <td>0.235778</td>\n",
       "      <td>0.065090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49298</th>\n",
       "      <td>0.793504</td>\n",
       "      <td>0.170653</td>\n",
       "      <td>0.035843</td>\n",
       "      <td>0.793504</td>\n",
       "      <td>0.170653</td>\n",
       "      <td>0.035843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49300</th>\n",
       "      <td>0.691009</td>\n",
       "      <td>0.237939</td>\n",
       "      <td>0.071052</td>\n",
       "      <td>0.691009</td>\n",
       "      <td>0.237939</td>\n",
       "      <td>0.071052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49301</th>\n",
       "      <td>0.791805</td>\n",
       "      <td>0.173843</td>\n",
       "      <td>0.034352</td>\n",
       "      <td>0.791805</td>\n",
       "      <td>0.173843</td>\n",
       "      <td>0.034352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49302</th>\n",
       "      <td>0.572137</td>\n",
       "      <td>0.307816</td>\n",
       "      <td>0.120047</td>\n",
       "      <td>0.572137</td>\n",
       "      <td>0.307816</td>\n",
       "      <td>0.120047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49307</th>\n",
       "      <td>0.696551</td>\n",
       "      <td>0.241778</td>\n",
       "      <td>0.061671</td>\n",
       "      <td>0.696551</td>\n",
       "      <td>0.241778</td>\n",
       "      <td>0.061671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49309</th>\n",
       "      <td>0.636832</td>\n",
       "      <td>0.255349</td>\n",
       "      <td>0.107819</td>\n",
       "      <td>0.636832</td>\n",
       "      <td>0.255349</td>\n",
       "      <td>0.107819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49313</th>\n",
       "      <td>0.771075</td>\n",
       "      <td>0.198047</td>\n",
       "      <td>0.030878</td>\n",
       "      <td>0.771075</td>\n",
       "      <td>0.198047</td>\n",
       "      <td>0.030878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49321</th>\n",
       "      <td>0.789945</td>\n",
       "      <td>0.173558</td>\n",
       "      <td>0.036497</td>\n",
       "      <td>0.789945</td>\n",
       "      <td>0.173558</td>\n",
       "      <td>0.036497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49323</th>\n",
       "      <td>0.669530</td>\n",
       "      <td>0.227823</td>\n",
       "      <td>0.102647</td>\n",
       "      <td>0.669530</td>\n",
       "      <td>0.227823</td>\n",
       "      <td>0.102647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49327</th>\n",
       "      <td>0.700053</td>\n",
       "      <td>0.214159</td>\n",
       "      <td>0.085787</td>\n",
       "      <td>0.700053</td>\n",
       "      <td>0.214159</td>\n",
       "      <td>0.085787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49328</th>\n",
       "      <td>0.672609</td>\n",
       "      <td>0.247510</td>\n",
       "      <td>0.079881</td>\n",
       "      <td>0.672609</td>\n",
       "      <td>0.247510</td>\n",
       "      <td>0.079881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49331</th>\n",
       "      <td>0.687414</td>\n",
       "      <td>0.235456</td>\n",
       "      <td>0.077130</td>\n",
       "      <td>0.687414</td>\n",
       "      <td>0.235456</td>\n",
       "      <td>0.077130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49333</th>\n",
       "      <td>0.733002</td>\n",
       "      <td>0.201328</td>\n",
       "      <td>0.065670</td>\n",
       "      <td>0.733002</td>\n",
       "      <td>0.201328</td>\n",
       "      <td>0.065670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49336</th>\n",
       "      <td>0.689634</td>\n",
       "      <td>0.223851</td>\n",
       "      <td>0.086515</td>\n",
       "      <td>0.689634</td>\n",
       "      <td>0.223851</td>\n",
       "      <td>0.086515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49343</th>\n",
       "      <td>0.710444</td>\n",
       "      <td>0.229694</td>\n",
       "      <td>0.059862</td>\n",
       "      <td>0.710444</td>\n",
       "      <td>0.229694</td>\n",
       "      <td>0.059862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49345</th>\n",
       "      <td>0.647773</td>\n",
       "      <td>0.267117</td>\n",
       "      <td>0.085110</td>\n",
       "      <td>0.647773</td>\n",
       "      <td>0.267117</td>\n",
       "      <td>0.085110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49348</th>\n",
       "      <td>0.697442</td>\n",
       "      <td>0.239351</td>\n",
       "      <td>0.063208</td>\n",
       "      <td>0.697442</td>\n",
       "      <td>0.239351</td>\n",
       "      <td>0.063208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49349</th>\n",
       "      <td>0.670048</td>\n",
       "      <td>0.253619</td>\n",
       "      <td>0.076333</td>\n",
       "      <td>0.670048</td>\n",
       "      <td>0.253619</td>\n",
       "      <td>0.076333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49350</th>\n",
       "      <td>0.689256</td>\n",
       "      <td>0.234480</td>\n",
       "      <td>0.076264</td>\n",
       "      <td>0.689256</td>\n",
       "      <td>0.234480</td>\n",
       "      <td>0.076264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14806 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr_low  lr_medium   lr_high    lr_low  lr_medium   lr_high\n",
       "5      0.754752   0.199560  0.045688  0.754752   0.199560  0.045688\n",
       "7      0.767030   0.191980  0.040989  0.767030   0.191980  0.040989\n",
       "11     0.722733   0.209716  0.067551  0.722733   0.209716  0.067551\n",
       "14     0.606957   0.273783  0.119260  0.606957   0.273783  0.119260\n",
       "22     0.605158   0.274803  0.120040  0.605158   0.274803  0.120040\n",
       "26     0.568870   0.331278  0.099852  0.568870   0.331278  0.099852\n",
       "27     0.580670   0.287763  0.131567  0.580670   0.287763  0.131567\n",
       "28     0.763173   0.196542  0.040285  0.763173   0.196542  0.040285\n",
       "30     0.788089   0.175586  0.036325  0.788089   0.175586  0.036325\n",
       "32     0.706143   0.227630  0.066228  0.706143   0.227630  0.066228\n",
       "33     0.648618   0.239978  0.111404  0.648618   0.239978  0.111404\n",
       "35     0.705901   0.235113  0.058986  0.705901   0.235113  0.058986\n",
       "40     0.619465   0.266144  0.114391  0.619465   0.266144  0.114391\n",
       "41     0.440288   0.366893  0.192819  0.440288   0.366893  0.192819\n",
       "42     0.450812   0.333215  0.215973  0.450812   0.333215  0.215973\n",
       "44     0.589071   0.284220  0.126708  0.589071   0.284220  0.126708\n",
       "47     0.499611   0.354675  0.145714  0.499611   0.354675  0.145714\n",
       "48     0.480811   0.337268  0.181921  0.480811   0.337268  0.181921\n",
       "66     0.776923   0.194684  0.028393  0.776923   0.194684  0.028393\n",
       "67     0.762961   0.205669  0.031370  0.762961   0.205669  0.031370\n",
       "70     0.529278   0.331641  0.139082  0.529278   0.331641  0.139082\n",
       "77     0.627941   0.294975  0.077084  0.627941   0.294975  0.077084\n",
       "78     0.893475   0.100422  0.006104  0.893475   0.100422  0.006104\n",
       "80     0.640884   0.282798  0.076318  0.640884   0.282798  0.076318\n",
       "97     0.517706   0.324035  0.158258  0.517706   0.324035  0.158258\n",
       "105    0.695699   0.240099  0.064202  0.695699   0.240099  0.064202\n",
       "107    0.559610   0.326922  0.113468  0.559610   0.326922  0.113468\n",
       "109    0.629339   0.260121  0.110541  0.629339   0.260121  0.110541\n",
       "110    0.590608   0.300260  0.109133  0.590608   0.300260  0.109133\n",
       "111    0.503730   0.328242  0.168028  0.503730   0.328242  0.168028\n",
       "...         ...        ...       ...       ...        ...       ...\n",
       "49267  0.565790   0.303253  0.130957  0.565790   0.303253  0.130957\n",
       "49269  0.594715   0.279239  0.126046  0.594715   0.279239  0.126046\n",
       "49271  0.491012   0.336680  0.172308  0.491012   0.336680  0.172308\n",
       "49276  0.627381   0.271441  0.101177  0.627381   0.271441  0.101177\n",
       "49277  0.839203   0.140196  0.020601  0.839203   0.140196  0.020601\n",
       "49278  0.472583   0.361601  0.165816  0.472583   0.361601  0.165816\n",
       "49279  0.514369   0.308372  0.177258  0.514369   0.308372  0.177258\n",
       "49281  0.698104   0.228922  0.072974  0.698104   0.228922  0.072974\n",
       "49284  0.686001   0.243596  0.070404  0.686001   0.243596  0.070404\n",
       "49286  0.956840   0.042004  0.001156  0.956840   0.042004  0.001156\n",
       "49293  0.699132   0.235778  0.065090  0.699132   0.235778  0.065090\n",
       "49298  0.793504   0.170653  0.035843  0.793504   0.170653  0.035843\n",
       "49300  0.691009   0.237939  0.071052  0.691009   0.237939  0.071052\n",
       "49301  0.791805   0.173843  0.034352  0.791805   0.173843  0.034352\n",
       "49302  0.572137   0.307816  0.120047  0.572137   0.307816  0.120047\n",
       "49307  0.696551   0.241778  0.061671  0.696551   0.241778  0.061671\n",
       "49309  0.636832   0.255349  0.107819  0.636832   0.255349  0.107819\n",
       "49313  0.771075   0.198047  0.030878  0.771075   0.198047  0.030878\n",
       "49321  0.789945   0.173558  0.036497  0.789945   0.173558  0.036497\n",
       "49323  0.669530   0.227823  0.102647  0.669530   0.227823  0.102647\n",
       "49327  0.700053   0.214159  0.085787  0.700053   0.214159  0.085787\n",
       "49328  0.672609   0.247510  0.079881  0.672609   0.247510  0.079881\n",
       "49331  0.687414   0.235456  0.077130  0.687414   0.235456  0.077130\n",
       "49333  0.733002   0.201328  0.065670  0.733002   0.201328  0.065670\n",
       "49336  0.689634   0.223851  0.086515  0.689634   0.223851  0.086515\n",
       "49343  0.710444   0.229694  0.059862  0.710444   0.229694  0.059862\n",
       "49345  0.647773   0.267117  0.085110  0.647773   0.267117  0.085110\n",
       "49348  0.697442   0.239351  0.063208  0.697442   0.239351  0.063208\n",
       "49349  0.670048   0.253619  0.076333  0.670048   0.253619  0.076333\n",
       "49350  0.689256   0.234480  0.076264  0.689256   0.234480  0.076264\n",
       "\n",
       "[14806 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=pd.DataFrame(lr(x_train,y_train1,x_test),columns=['lr_low','lr_medium','lr_high'],index=x_test.index)\n",
    "pd.concat([A, A], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#base model 1: multinomial logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "def mlog(x_train,y_train,x_test):\n",
    "    lr = LogisticRegression().fit(x_train, y_train)\n",
    "    return lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6960691611508848"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###accuracy on total train and test\n",
    "accuracy_score(mlog(x_train,y_train,x_test),test.loc[:,'interest_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#base model 2: bagged decision trees\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def bagDT(x_train,y_train,x_test,y_test):\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    cart = DecisionTreeClassifier()\n",
    "    num_trees = 100\n",
    "    model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees)\n",
    "    model.fit(x_train,y_train)\n",
    "    predict = model_selection.cross_val_predict(model, x_test, y_test, cv=kfold)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'low', u'low', u'low', ..., u'low', u'low', u'low'], dtype=object)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagDT(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#base model 3: Random Forest trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "seed = 7\n",
    "def rfClassifier(x_train,y_train,x_test,y_test):\n",
    "    num_trees = 100\n",
    "    max_features = 3\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "    model.fit(x_train,y_train)\n",
    "    predicted = model_selection.cross_val_predict(model, x_test, y_test, cv=kfold)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#base model 4: SVM\n",
    "#use cross validation to select cost, cost(default)=1\n",
    "from sklearn import svm\n",
    "def svmm(x_train,y_train,x_test):\n",
    "    clf = svm.SVC(decision_function_shape='ovr')\n",
    "    clf.fit(x_train, y_train) \n",
    "    return clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#accuracy_score(svmm(x_train,y_train,x_test),test_data.loc[:,'interest_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#base model 5: Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "def mnb(x_train,y_train,x_test):\n",
    "    gnb=GaussianNB()\n",
    "    y_pred = gnb.fit(x_train, y_train).predict(x_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#base model 6: Ada boosting classifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "def adaBC(x_train,y_train,x_test):\n",
    "    clf = AdaBoostClassifier(n_estimators=100)\n",
    "    clf.fit(x_train,y_train)\n",
    "    return clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################now we ensemble them ###########################################################\n",
    "#1st: partition the train set into 6 test sets\n",
    "k=x_train.shape[0]/5\n",
    "x_sp=[[],[],[],[],[]]\n",
    "for i in range(4):\n",
    "    sample=random.sample(x_train.index,k)\n",
    "    x_sp[i]=x_train.ix[sample]\n",
    "    x_train=x_train.drop(sample)\n",
    "x_sp[4]=x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2nd: create train_meta and test_meta\n",
    "train_meta=pd.DataFrame()\n",
    "train_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3rd: for each fold in 1st, use other 5 folds as training set to predict the result for that fold.\n",
    "#and save them in train_meta\n",
    "x_train=train.drop('interest_level',axis=1)\n",
    "for i in range(5):\n",
    "    x_sub_test=x_sp[i]\n",
    "    x_sub_train=x_train.drop(x_sub_test.index)\n",
    "    y_sub_test=y_train[x_sub_test.index]\n",
    "    y_sub_train=y_train[x_sub_train.index]\n",
    "    M1=pd.Series(mlog(x_sub_train,y_sub_train,x_sub_test),index=x_sub_test.index)\n",
    "    M2=pd.Series(bagDT(x_sub_train,y_sub_train,x_sub_test,y_sub_test),index=x_sub_test.index)\n",
    "    M3=pd.Series(rfClassifier(x_sub_train,y_sub_train,x_sub_test,y_sub_test),index=x_sub_test.index)\n",
    "    M4=pd.Series(svmm(x_sub_train,y_sub_train,x_sub_test),index=x_sub_test.index)\n",
    "    M5=pd.Series(mnb(x_sub_train,y_sub_train,x_sub_test),index=x_sub_test.index)\n",
    "    M6=pd.Series(adaBC(x_sub_train,y_sub_train,x_sub_test),index=x_sub_test.index)\n",
    "    app={'M1':M1,'M2':M2,'M3':M3, 'M4':M4, 'M5':M5,'M6':M6}\n",
    "    train_meta=train_meta.append(pd.DataFrame(app))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#4th:Fit each base model to the full training dataset \n",
    "#and make predictions on the test dataset. Store these predictions inside test_meta\n",
    "M1=pd.Series(mlog(x_train,y_train,x_test),index=x_test.index)\n",
    "M2=pd.Series(bagDT(x_train,y_train,x_test,y_test),index=x_test.index)\n",
    "M3=pd.Series(rfClassifier(x_train,y_train,x_test,y_test),index=x_test.index)\n",
    "M4=pd.Series(svmm(x_train,y_train,x_test),index=x_test.index)\n",
    "M5=pd.Series(mnb(x_train,y_train,x_test),index=x_test.index)\n",
    "M6=pd.Series(adaBC(x_train,y_train,x_test),index=x_test.index)\n",
    "test['M1']=M1\n",
    "test['M2']=M2\n",
    "test['M3']=M3\n",
    "test['M4']=M4\n",
    "test['M5']=M5\n",
    "test['M6']=M6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#5th: Fit a new model, S (i.e the stacking model) to train_meta, using M1 and M2 as features.\n",
    "#Optionally, include other features from the original training dataset or engineered features\n",
    "##==> transfer to dummy variables\n",
    "train_meta_dummy=pd.get_dummies(train_meta)\n",
    "test_meta=test.loc[:,['M1','M2','M3','M4','M5','M6']]\n",
    "test_meta_dummy=pd.get_dummies(test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.707078211536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "low       13024\n",
       "medium     1234\n",
       "high        548\n",
       "dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest with meta only\n",
    "res=rfClassifier(train_meta_dummy,y_train,test_meta_dummy,y_test)\n",
    "print accuracy_score(res,y_test)\n",
    "pd.Series(res).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####concate meta to original dataset and train it again########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_last_train=pd.concat([train_meta_dummy,x_train],axis=1)\n",
    "x_last_test=pd.concat([test_meta_dummy,x_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.718357422666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "low       11999\n",
       "medium     2027\n",
       "high        780\n",
       "dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest with combined\n",
    "res=rfClassifier(x_last_train,y_train,x_last_test,y_test)\n",
    "print accuracy_score(res,y_test)\n",
    "pd.Series(res).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.to_json(\"train_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.to_json(\"test_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_last_train.to_json('train_meta_data.json')\n",
    "x_last_test.to_json('test_meta_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance=['price','avg_imagesize','avg_luminance','avg_brightness','img_quantity','bedrooms','bathrooms','No Fee',\\\n",
    "\t'Elevator','Doorman','Laundry In Building','Fitness Center','Exclusive','Cats Allowed','Dogs Allowed','Reduced Fee','Funished',\\\n",
    "\t'Laundry In Unit','Common Outdoor Space','Private Outdoor Space','Parking Space','Short Term Allowed','By Owner','Sublet / Lease-Break',\\\n",
    "\t'Storage Facility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
